import math
import numpy as np
import pandas as pd
from scipy import stats
import statistics

import matplotlib.pyplot as plt
import matplotlib as mpl
import seaborn as sns

#from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.feature_selection import f_classif
from sklearn.manifold import TSNE

from sklearn.metrics import silhouette_samples, silhouette_score
from sklearn.cluster import KMeans

from sklearn.preprocessing import label_binarize
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import RocCurveDisplay

from sklearn.model_selection import cross_validate

from sklearn.pipeline import make_pipeline
from sklearn.metrics import homogeneity_score, completeness_score, v_measure_score, adjusted_rand_score, adjusted_mutual_info_score, silhouette_score

from sklearn.neighbors import kneighbors_graph
from sklearn.model_selection import StratifiedKFold
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score
from sklearn.model_selection import cross_val_score

import warnings
warnings.filterwarnings("ignore")

#colors = ['red','cyan','lime','darkgreen','gold','pink','lightsalmon','yellow','maroon','blue','teal','orange','olive','indigo','fuchsia','palegreen','crimson','navy','black']
colors = ['red','blue','yellow','darkgreen','orange','pink','lime','maroon','cyan','gold','teal','lightsalmon','olive','indigo','fuchsia','palegreen','crimson','navy','black']

def pca_proc(res_df, id_column='id', n_components=100, plot=False):
    data_proc = res_df.drop(id_column, axis=1, errors = 'ignore')
    pca = PCA(n_components=n_components)
    pca.fit(StandardScaler().fit_transform(data_proc))
    
    if plot == True:
        plt.plot(pca.explained_variance_ratio_, 'bx')
        plt.xscale('log')
        plt.ylabel('Explained Variance')
        plt.xlabel('Components')
        plt.xlim(.9, pca.n_components_)
    
    pca_data = pd.DataFrame(pca.transform(data_proc))
    
    pca_data = pca_data.rename(columns= lambda x: f'PC{x}')
    
    pca_data[id_column] = res_df[id_column]
    
    return pca_data


def tsne_proc(proc_df, id_column='id', init='pca', random_state=7, perplexity=15):
    X_embedded = TSNE(n_components=2,init = init,
                         random_state = random_state, perplexity=perplexity).fit_transform(proc_df.drop(id_column, axis=1, errors = 'ignore'))
    tsne_data = pd.DataFrame(data = X_embedded, columns = ['DM1', 'DM2'])
    
    tsne_data[id_column] = proc_df[id_column]
    return tsne_data

def pc_anova(data, pc_n, group):
    pc_anova = pd.DataFrame(columns=['pc','F','pvalue'])

    for n in range(0,pc_n):
        F,pval = f_classif(np.array(data[n]).reshape(-1,1),data[group])
        pc_anova = pd.concat([pc_anova,pd.DataFrame(data={'pc': [n] ,'F': [float(F)],'pvalue':[float(pval)]})])
    pc_anova = pc_anova.sort_values('pvalue')
    return pc_anova

def make_custom_palette(labels_list):
    color_n = len(labels_list)
    #palette = sns.color_palette("bright", color_n).as_hex()
    palette = colors[0:color_n]
    custom_palette = {labels_list[i]: palette[i] for i in range(color_n)}
    custom_palette['other']='lightgrey'
    return custom_palette

#def tsne_plot(data_plot, to_color, title, suptitle=None,legend=True, custom_palette=None,ax=None,):
#        
#    if ax is None:
#        fig, ax = plt.subplots(figsize=(7, 7))
#    sns.scatterplot(x='DM1', y='DM2', data=data_plot.sort_values(to_color), hue=to_color,s=8, legend = legend,palette=custom_palette, ax=ax)
#    #plt.suptitle(suptitle)
#    ax.set_title(title)
#    ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)

def tsne_plot(data_plot, to_color, title, to_size=None,suptitle=None,legend=True, custom_palette=None,ax=None,):
    if ax is None:
        fig, ax = plt.subplots(figsize=(7, 7))
    if to_size is None:
        sns.scatterplot(x='DM1', y='DM2', data=data_plot.sort_values(to_color), hue=to_color, s=2,legend = legend,palette=custom_palette, ax=ax)
    else:
        sns.scatterplot(x='DM1', y='DM2', data=data_plot.sort_values(to_color), hue=to_color,size=to_size, sizes=(2, 100),legend = legend,palette=custom_palette, ax=ax)
    ax.set_title(title)
    ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)

## Metrics
### to check fraction_matched_exp
def binominal_test(df, cluster, group, threshold = 0.7):
    binom_df = df.copy()
    binom_df['total_cluster'] = binom_df.groupby(cluster)[cluster].transform('count')
    binom_df['total_group'] = binom_df.groupby(group)[group].transform('count')
    binom_df['count_matched'] = binom_df.groupby([group,cluster])[group].transform('count')
    binom_df['fraction_matched'] = binom_df['count_matched']/binom_df['total_cluster']
    binom_df['fraction_matched_exp'] = binom_df['total_group']/len(binom_df.index)
    binom_df['p_value'] = binom_df.apply(lambda row: stats.binomtest(row['count_matched'], n=row['total_cluster'], p=row['fraction_matched_exp'], alternative='greater').pvalue,axis=1)
    #binom_df = binom_df[binom_df['fraction_matched']>binom_df['fraction_matched_exp']]
    binom_df_cluster = binom_df[[group, cluster,'total_cluster','total_group','count_matched','fraction_matched','fraction_matched_exp','p_value']].drop_duplicates().sort_values('p_value')
    binom_df_cluster['is_cluster'] = binom_df_cluster.apply(lambda x: 1 if (x.total_cluster>1) and (x.cluster!=-1) else 0,axis=1)
    #binom_df_cluster['is_cluster'] = binom_df_cluster['total_cluster'].apply(lambda x: 1 if x>1 else 0)
    binom_df_cluster['enriched_clstr'] = binom_df_cluster.apply(lambda x:1 
                                                                if (x.fraction_matched>=threshold)
                                                                and (x.is_cluster==1) else 0,axis=1)
    binom_df_cluster = binom_df_cluster.sort_values(['fraction_matched'],ascending=False)
    binom_df_cluster = binom_df_cluster.drop_duplicates('cluster',keep='first')
    return binom_df_cluster

def clsf_metrics(y_test,pred,average='micro'):
    clsf_metrics = {
        'f1': f1_score(y_test, pred, average = average),
        'precision' : precision_score(y_test, pred, average=average),
        'recall':recall_score(y_test, pred, average=average),
    }
    return clsf_metrics


def count_clstr_purity(binom_res):
    binom_res_clstr = binom_res[binom_res['is_cluster']==1]
    if len(binom_res_clstr)!=0:
        return sum(binom_res_clstr['count_matched'])/sum(binom_res_clstr['total_cluster'])


def clstr_metrics(y_data, labels):
    clstr_metrics = {
        'homogeneity_score' : homogeneity_score(y_data, labels),
        'completeness_score' : completeness_score(y_data, labels),
        'v_measure_score' : v_measure_score(y_data, labels),
        'adjusted_rand_score' : adjusted_rand_score(y_data, labels),
        'adjusted_mutual_info_score' : adjusted_mutual_info_score(y_data, labels),
    }
    return clstr_metrics    
    
## Clustering
def clstr_model(model, proc_df, id_column):
    model.fit(proc_df.drop(id_column, axis=1, errors = 'ignore'))    
    clustering_data = pd.DataFrame(model.labels_, columns= ['cluster'])
    
    clustering_data[id_column] = proc_df[id_column]
    
    return clustering_data, model

def kmeans_proc(proc_df, id_column, n_clusters, random_state):
    kmeans = KMeans(n_clusters=n_clusters, random_state=random_state).fit(proc_df.drop(id_column, axis=1, errors = 'ignore'))    
    kmeans_data = pd.DataFrame(kmeans.labels_, columns= ['cluster'])
    
    kmeans_data[id_column] = proc_df[id_column]
    
    return kmeans_data


##Classification
def generate_negative_pairs(positive_pairs, n , p1_col, p2_col):
    negative_pairs = []
    i=0
    while i < n:
        p1 = positive_pairs.sample(n=1).reset_index(drop=True)
        p2 = positive_pairs.sample(n=1).reset_index(drop=True)
        pair = {}
        pair[p1_col] = p1[p1_col][0]
        pair[p2_col] = p2[p2_col][0]
        if (len(positive_pairs[(positive_pairs[p1_col]==pair[p1_col])&(positive_pairs[p2_col]==pair[p2_col])])==0) and (pair not in negative_pairs):
            negative_pairs.append(pair)
            i += 1
    return pd.DataFrame(negative_pairs)

def roc_auc_count_binary(y_test,y_pred):
    fpr, tpr, _ = roc_curve(y_test,y_pred)
    return auc(fpr,tpr)

def roc_auc_count(y_test_curv,y_pred_curv):
    n_classes = y_test_curv.shape[1]
    fpr = dict()
    tpr = dict()
    roc_auc = dict()
    for i in range(n_classes):
        fpr[i], tpr[i], _ = roc_curve(y_test_curv[:,i],y_pred_curv[:,i])
        roc_auc[i] = auc(fpr[i], tpr[i])
    return roc_auc

def plot_roccurve_multi(classes_list,y_test_curv,y_score,title, ax,custom_palette=None,test_acc=None, f1_weighted=None,show_legend=True):
    if ax is None:
        fig, ax = plt.subplots(figsize=(7, 7))
    for class_id in range(len(classes_list)):
        if custom_palette is None:
            color = None#colors[class_id]
        else:
            color = custom_palette[classes_list[class_id]]
        if sum(y_test_curv[:, class_id])!=0:
            RocCurveDisplay.from_predictions(
                y_test_curv[:, class_id],
                y_score[:, class_id],
                name=f"ROC curve for {classes_list[class_id]}",
                ax=ax,
                color = color,
                plot_chance_level=(class_id == 2),)
    if f1_weighted is not None:
        print(f"accuracy: {round(test_acc,2)}\nf1_weighted:{round(f1_weighted['f1'],2)}\nprecision:{round(f1_weighted['precision'],2)}\nrecall:{round(f1_weighted['recall'],2)}")
        #ax.text(1.7,0.2,f"accuracy: {round(test_acc,2)}\nf1_weighted:{round(f1_weighted['f1'],2)}\nprecision:{round(f1_weighted['precision'],2)}\nrecall:{round(f1_weighted['recall'],2)}")
    ax.set_title(title)
    if show_legend:
        ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)
    else: ax.get_legend().remove()

    
##Evaluation
def silhouette_avg_scores_kmeans(X, range_n_clusters):
    silhouette_avgs = {}
    for n_clusters in range_n_clusters:
        clusterer = KMeans(n_clusters=n_clusters, n_init="auto", random_state=10)
        cluster_labels = clusterer.fit_predict(X)
        
        silhouette_avg = silhouette_score(X, cluster_labels)
        print(
            "For n_clusters =",
            n_clusters,
            "The average silhouette_score is :",
            silhouette_avg,
        )
        silhouette_avgs[n_clusters]=silhouette_avg
    return silhouette_avgs

def silhouette_avg_scores_model(X, range_n_clusters, model_name):
    silhouette_avgs = {}
    for n_clusters in range_n_clusters:
        #clusterer = KMeans(n_clusters=n_clusters, n_init="auto", random_state=10)
        clusterer = init_clstr_model(model_name, pd.Series(data=[n_clusters], index=['n_clusters']))
        cluster_labels = clusterer.fit_predict(X)
        
        silhouette_avg = silhouette_score(X, cluster_labels)
        print(
            "For n_clusters =",
            n_clusters,
            "The average silhouette_score is :",
            silhouette_avg,
        )
        silhouette_avgs[n_clusters]=silhouette_avg
    return silhouette_avgs

import matplotlib.cm as cm

def silhouette_avg_scores_kmeans_with_plot(X, range_n_clusters):
    silhouette_avgs = {}
    for n_clusters in range_n_clusters:
        
        fig, (ax1) = plt.subplots(1,1)
        fig.set_size_inches(18, 7)
        
        ax1.set_xlim([-0.1, 1])
        ax1.set_ylim([0, len(X) + (n_clusters + 1) * 10])

        
        clusterer = KMeans(n_clusters=n_clusters, n_init="auto", random_state=10)
        cluster_labels = clusterer.fit_predict(X)
        
        silhouette_avg = silhouette_score(X, cluster_labels)
        print(
            "For n_clusters =",
            n_clusters,
            "The average silhouette_score is :",
            silhouette_avg,
        )
        silhouette_avgs[n_clusters]=silhouette_avg
        
        sample_silhouette_values = silhouette_samples(X, cluster_labels)
        
        y_lower = 10
        for i in range(n_clusters):
            ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]
            
            ith_cluster_silhouette_values.sort()
            
            size_cluster_i = ith_cluster_silhouette_values.shape[0]
            y_upper = y_lower + size_cluster_i
            
            color = cm.nipy_spectral(float(i) / n_clusters)
            
            ax1.fill_betweenx(
                np.arange(y_lower, y_upper),
                0,
                ith_cluster_silhouette_values,
                facecolor=color,
                edgecolor=color,
                alpha=0.7,
            )

            # Label the silhouette plots with their cluster numbers at the middle
            ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))
            
            # Compute the new y_lower for next plot
            y_lower = y_upper + 10  # 10 for the 0 samples
        
        ax1.set_title("The silhouette plot for the various clusters. N=" + str(n_clusters))
        ax1.set_xlabel("The silhouette coefficient values")
        ax1.set_ylabel("Cluster label")

        # The vertical line for average silhouette score of all the values
        ax1.axvline(x=silhouette_avg, color="red", linestyle="--")

        ax1.set_yticks([])  # Clear the yaxis labels / ticks
        ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])

    plt.show()
        
    return silhouette_avgs


def clf_cross_validate(clfs, X_train, y_train):
    for classifier in clfs:
        steps = [('model', classifier)]
        pipeline = Pipeline(steps=steps)
        #pipeline.set_params(clf = classifier)
        scores = cross_validate(pipeline, X_train, y_train)
        print('---------------------------------')
        print(str(classifier))
        print('-----------------------------------')
        for key, values in scores.items():
            print(key,' mean ', values.mean())
            print(key,' std ', values.std())
            

def connectivity_kneighbors_graph(X, n_neighbors): 
    connectivity = kneighbors_graph(X, n_neighbors=n_neighbors, include_self=False)
    # make connectivity symmetric
    connectivity = 0.5 * (connectivity + connectivity.T)
    return connectivity

def init_clstr_model(model_name, r):
    if model_name == 'KMeans':
        from sklearn.cluster import KMeans
        clstr = KMeans(n_clusters=r.n_clusters, random_state=12)
    elif model_name == 'Birch':
        from sklearn.cluster import Birch
        clstr = Birch(n_clusters=r.n_clusters)
    elif model_name == 'DBSCAN':
        from sklearn.cluster import DBSCAN
        clstr = DBSCAN(eps=r.eps)
    elif model_name == 'Agglomerativ':
        from sklearn.cluster import AgglomerativeClustering
        clstr = AgglomerativeClustering(n_clusters=r.n_clusters, linkage=r.linkage, connectivity=r.connectivity)
    else:
        raise RuntimeError('Unknown clusterer')
    return clstr

def bench_clustering(X_data, y_data, model, to_scale=False):
    #t0 = time()
    if to_scale == True:
        estimator = make_pipeline(StandardScaler(), model).fit(data_X)
    else:
        estimator = make_pipeline(model).fit(X_data)
        
    #fit_time = time() - t0       
    #results = [model_name, fit_time, estimator[-1].inertia_]

    clustering_metrics = {
        #'time' : fit_time, 
        #'inertia' : estimator[-1].inertia_, ## Birch does not have it
        'homogeneity_score' : homogeneity_score(y_data, estimator[-1].labels_),
        'completeness_score' : completeness_score(y_data, estimator[-1].labels_),
        'v_measure_score' : v_measure_score(y_data, estimator[-1].labels_),
        'adjusted_rand_score' : adjusted_rand_score(y_data, estimator[-1].labels_),
        'adjusted_mutual_info_score' : adjusted_mutual_info_score(y_data, estimator[-1].labels_),
    }
    
    clustering_metrics['silhouette_score'] = silhouette_score(X_data,estimator[-1].labels_
    )
    return clustering_metrics

def run_bench_clustering(y_data, X_data, clstr_params):
    clstr_scores = []
    for model_name in clstr_params.keys():
        paramms = clstr_params[model_name]
        models = pd.DataFrame(paramms).apply(lambda x: init_clstr_model(model_name,x),axis = 1)
        for model in models:
            model_results = bench_clustering(X_data, y_data, model)
            model_results['model'] = str(model)
            clstr_scores.append(model_results)
    return clstr_scores


def init_clf_model(model):
    if model == 'LR':
        from sklearn.linear_model import LogisticRegression
        clf = LogisticRegression(random_state=42)
    elif model == 'SVM':
        from sklearn.svm import SVC
        clf = SVC()
    elif model == 'LinearSVC':
        from sklearn.svm import LinearSVC
        clf = LinearSVC()
    elif model == 'SVM_ovr':
        from sklearn.svm import SVC
        from sklearn.multiclass import OneVsRestClassifier
        clf = OneVsRestClassifier(SVC())
    elif model == 'AB':
        from sklearn.ensemble import AdaBoostClassifier
        clf = AdaBoostClassifier()
    elif model == 'KNN':
        from sklearn.neighbors import KNeighborsClassifier
        clf = KNeighborsClassifier()
    elif model == 'RF':
        from sklearn.ensemble import RandomForestClassifier
        clf = RandomForestClassifier()
    elif model == 'xgboost':
        from xgboost import XGBClassifier
        clf = XGBClassifier()
    elif model == 'mlpclassifier':
        from sklearn.neural_network import MLPClassifier
        clf = MLPClassifier()
    else:
        raise RuntimeError('Unknown classifier')
    return clf

def clf_evaluate_models(X_train, y_train, X_test, y_test, model_params, n_splits = 5, f1_averaged='micro',  scoring_func=None, debug=False):
    
    skf = StratifiedKFold(n_splits=n_splits, random_state=42, shuffle=True)
    best_clfs = {}
    scores = {'model': [], 'f1': [], 'precision': [] , 'recall': [], 'train_accuracy': [], 'test_accuracy': [], 'params': []}
    all_scores = {}
    model_names = list(model_params.keys())
    for model_name in model_names:
        if debug:
            print(f'Started evaluating {model_name}')
        model = init_clf_model(model_name)
        clf_steps = [(model_name, model)]

        pipe = Pipeline(steps=clf_steps)
        clf = GridSearchCV(pipe, model_params[model_name], scoring=scoring_func, cv=skf, n_jobs=-1)
        clf.fit(X_train, y_train)
        
        best_model = clf.best_estimator_[model_name]
        pred = best_model.predict(X_test)
        
        f1 = f1_score(y_test, pred, average = f1_averaged)
        score_train = best_model.score(X_train,y_train)
        score_test = best_model.score(X_test,y_test)
        precision = precision_score(y_test, pred, average=f1_averaged)
        recall = recall_score(y_test, pred, average=f1_averaged)
        
        scores['model'].append(model_name)
        scores['f1'].append(f1)
        scores['precision'].append(precision)
        scores['recall'].append(recall)
        scores['train_accuracy'].append(score_train)
        scores['test_accuracy'].append(score_test)
        scores['params'].append(clf.best_params_)
        all_scores[model_name]=clf.cv_results_
        if debug:
            print(f'Best params for {model_name}:', clf.best_params_)
            print(f'scores for the best model: f1 = {f1} precision = {precision} recall = {recall} train_accuracy = {score_train} test_accuracy = {score_test}')
            print()

        best_clfs[model_name] = clf.best_estimator_
    print(pd.DataFrame({'classifier': scores['model'],
                        'f1': scores['f1'],
                        'precision': scores['precision'],
                        'recall': scores['recall'],
                        'train_accuracy' : scores['train_accuracy'],
                        'test_accuracy' : scores['test_accuracy']
                        }).set_index('classifier').T)
    #best_model_idx = max(range(len(scores)), key=lambda i: scores[i])
    #print(f'Best model is {model_names[best_model_idx]} with params: {params[best_model_idx]}')

    return best_clfs, scores, all_scores

